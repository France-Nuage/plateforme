â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          GITLAB CI OPTIMIZATION PLAN - KUBERNETES + EMPTYDIR RUNNER          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CONTEXT:
- Monorepo avec Rust (controlplane) + Node.js (console, node-sdk, system-tests)
- Runner K8s avec emptyDir (full ephemeral, pas de cache local)
- StratÃ©gie: maximiser parallÃ©lisation + registry cache

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… PHASE 1 - COMPLETED (Quick Wins)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. GIT_DEPTH: 1 par dÃ©faut
   - Gain: ~1-2 min (30-60s Ã— ~20 jobs)
   - Override pour migration linting et release

2. needs: dependencies
   - Gain: ~5-10 min
   - Jobs dÃ©marrent dÃ¨s que LEUR dÃ©pendance est prÃªte
   - Exemples:
     * code style / controlplane â†’ needs: build / controlplane
     * unit tests / authz â†’ needs: [] (image externe)

TOTAL PHASE 1: 6-12 minutes

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â³ PHASE 2 - PLANNED (Medium Impact)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. PARALLÃ‰LISER "build / release"
   Gain: 15-20 min

   Action:
   - Ã‰clater le job monolithique en jobs sÃ©parÃ©s par service
   - Tous les builds release tournent en parallÃ¨le

   Avant:
     "build / release" (1 job sÃ©quentiel)
       â”œâ”€ controlplane
       â”œâ”€ authz-worker
       â”œâ”€ synchronizer
       â”œâ”€ console
       â”œâ”€ system-tests
       â””â”€ protocol

   AprÃ¨s:
     6 jobs parallÃ¨les:
       â”œâ”€ "build / controlplane / release"
       â”œâ”€ "build / authz-worker / release"
       â”œâ”€ "build / synchronizer / release"
       â”œâ”€ "build / console / release"
       â”œâ”€ "build / system-tests / release"
       â””â”€ "build / protocol / release"

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

2. OPTIMISER RUST MULTI-BINARY BUILDS
   Gain: 10-15 min

   ProblÃ¨me:
   - 3 Dockerfiles sÃ©parÃ©s pour controlplane (server, authz-worker, synchronizer)
   - Chaque build recompile le workspace complet
   - cargo chef cook s'exÃ©cute 3 fois

   Solution:
   - CrÃ©er controlplane/Dockerfile.all qui build les 3 binaires en 1 passe
   - Utiliser --target pour crÃ©er 3 images sÃ©parÃ©es

   Dockerfile.all:
   ```dockerfile
   FROM rust:1.90-alpine AS chef
   # ... setup

   FROM chef AS builder
   RUN SQLX_OFFLINE=true cargo build --release \
       --bin server \
       --bin authz-worker \
       --bin synchronizer

   FROM alpine:3.23 AS server
   COPY --from=builder /app/target/release/server /usr/bin/server
   CMD ["/usr/bin/server"]

   FROM alpine:3.23 AS authz-worker
   COPY --from=builder /app/target/release/authz-worker /usr/bin/authz-worker
   CMD ["/usr/bin/authz-worker"]

   FROM alpine:3.23 AS synchronizer
   COPY --from=builder /app/target/release/synchronizer /usr/bin/synchronizer
   CMD ["/usr/bin/synchronizer"]
   ```

   CI usage:
   ```yaml
   "build / controlplane-all":
     script:
       - |
         docker buildx build \
           --target server \
           --tag ${CI_REGISTRY_IMAGE}/controlplane:${CI_COMMIT_REF_SLUG}-development \
           --target authz-worker \
           --tag ${CI_REGISTRY_IMAGE}/authz-worker:${CI_COMMIT_REF_SLUG}-development \
           --target synchronizer \
           --tag ${CI_REGISTRY_IMAGE}/synchronizer:${CI_COMMIT_REF_SLUG}-development \
           --push --file controlplane/Dockerfile.all controlplane
   ```

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

3. CACHE REGISTRY AMÃ‰LIORÃ‰
   Gain: 2-5 min

   Ajout de refs par branche pour meilleur hit rate:

   ```yaml
   --cache-from type=registry,ref=${CI_REGISTRY_IMAGE}/service:buildcache \
   --cache-from type=registry,ref=${CI_REGISTRY_IMAGE}/service:buildcache-${CI_COMMIT_REF_SLUG} \
   --cache-from type=registry,ref=${CI_REGISTRY_IMAGE}/service:${CI_COMMIT_REF_SLUG}-dev \
   --cache-from type=registry,ref=${CI_REGISTRY_IMAGE}/service:master-dev \
   --cache-to type=registry,ref=${CI_REGISTRY_IMAGE}/service:buildcache-${CI_COMMIT_REF_SLUG},mode=max \
   --cache-to type=registry,ref=${CI_REGISTRY_IMAGE}/service:buildcache,mode=max
   ```

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

4. OPTIMISER CONTEXTES DOCKER
   Gain: 30s-1min

   ProblÃ¨me:
   - console/Dockerfile utilise contexte "." (1.5GB envoyÃ© au daemon)
   - system-tests/Dockerfile idem
   - Raison: ils buildent node-sdk en interne

   Solution A (simple):
   - Extraire node-sdk en stage nommÃ©
   - Utiliser l'image du registry dans console/system-tests

   Solution B (avancÃ©e):
   - Utiliser --cache-from avec l'image node-sdk
   - COPY --from pour rÃ©cupÃ©rer les artifacts

   Exemple Solution A:
   ```dockerfile
   # console/Dockerfile
   ARG NODE_SDK_IMAGE
   FROM ${NODE_SDK_IMAGE} AS node-sdk

   FROM node:22-alpine AS base
   COPY --from=node-sdk /app /node-sdk

   WORKDIR /app
   COPY package.json package-lock.json ./
   RUN npm ci
   COPY . .
   # ...
   ```

   CI:
   ```yaml
   "build / console":
     script:
       - |
         docker buildx build \
           --build-arg NODE_SDK_IMAGE=${CI_REGISTRY_IMAGE}/node-sdk:${CI_COMMIT_REF_SLUG} \
           --file console/Dockerfile \
           console  # Contexte rÃ©duit Ã  console/
   ```

TOTAL PHASE 2: 28-41 minutes

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸš€ PHASE 3 - PLANNED (Advanced Infrastructure)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. REGISTRY PULL-THROUGH CACHE
   Gain: 5-10 min

   Infrastructure:
   - DÃ©ployer registry:2 dans le cluster K8s
   - Configurer en mode mirror/proxy
   - Pointer runners vers ce registry local

   registry-mirror.yaml:
   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: docker-registry-mirror
     namespace: gitlab-runner
   spec:
     replicas: 2
     selector:
       matchLabels:
         app: registry-mirror
     template:
       metadata:
         labels:
           app: registry-mirror
       spec:
         containers:
         - name: registry
           image: registry:2
           env:
           - name: REGISTRY_PROXY_REMOTEURL
             value: https://registry.gitlab.com
           - name: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY
             value: /var/lib/registry
           ports:
           - containerPort: 5000
           volumeMounts:
           - name: registry-storage
             mountPath: /var/lib/registry
         volumes:
         - name: registry-storage
           persistentVolumeClaim:
             claimName: registry-mirror-pvc
   ---
   apiVersion: v1
   kind: Service
   metadata:
     name: registry-mirror
     namespace: gitlab-runner
   spec:
     selector:
       app: registry-mirror
     ports:
     - port: 5000
       targetPort: 5000
   ```

   Runner config.toml:
   ```toml
   [[runners]]
     [runners.kubernetes]
       [[runners.kubernetes.volumes.host_path]]
         name = "docker-daemon-config"
         mount_path = "/etc/docker"
         host_path = "/etc/docker"
   ```

   /etc/docker/daemon.json sur les nodes:
   ```json
   {
     "registry-mirrors": ["http://registry-mirror.gitlab-runner.svc.cluster.local:5000"]
   }
   ```

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

2. BUILDKIT REMOTE CACHE (S3/GCS)
   Gain: 3-7 min

   Plus efficace que inline cache registry:
   - Meilleure compression
   - MÃ©tadonnÃ©es sÃ©parÃ©es
   - Pas besoin de pull l'image complÃ¨te

   PrÃ©requis:
   - Bucket S3 ou GCS
   - Credentials configurÃ©es dans le runner

   CI usage:
   ```yaml
   script:
     - |
       docker buildx build \
         --cache-from type=s3,region=eu-west-1,bucket=gitlab-ci-cache,name=controlplane \
         --cache-to type=s3,region=eu-west-1,bucket=gitlab-ci-cache,name=controlplane,mode=max \
         ...
   ```

   Alternative GCS:
   ```yaml
   --cache-from type=gha \  # GitHub Actions cache (si on migre)
   --cache-to type=gha,mode=max
   ```

TOTAL PHASE 3: 8-17 minutes

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š RÃ‰CAPITULATIF DES GAINS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Phase 1 (DONE):      6-12 min   âœ…
Phase 2 (PLANNED):  28-41 min   â³
Phase 3 (PLANNED):   8-17 min   ğŸš€
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL POTENTIEL:    42-70 min

Pipeline actuelle estimÃ©e: ~60-80 min
Pipeline optimisÃ©e cible:   ~10-40 min

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ” MÃ‰TRIQUES Ã€ SURVEILLER POST-PHASE 1
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Temps de clone git
   - Attendu: <10s au lieu de ~60s
   - Sauf migration linting et release: ~60s OK

2. Temps d'attente entre stages
   - code-style jobs doivent dÃ©marrer dÃ¨s leur build fini
   - unit-tests avec needs:[] dÃ©marrent immÃ©diatement

3. Cache hit rate
   - VÃ©rifier dans les logs buildx
   - "CACHED" doit apparaÃ®tre pour les layers

4. Erreurs potentielles
   - Migration linting: vÃ©rifier accÃ¨s Ã  origin/master
   - Semantic-release: vÃ©rifier historique git complet
   - Jobs avec needs: vÃ©rifier que les dÃ©pendances existent

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ NOTES TECHNIQUES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Pourquoi GIT_STRATEGY=fetch au lieu de clone?
- fetch rÃ©utilise le repo si disponible (pas le cas avec emptyDir)
- Mais prÃ©pare le terrain si on passe Ã  un runner avec cache

Pourquoi needs: artifacts: false?
- On ne transfÃ¨re que les mÃ©tadonnÃ©es de job
- Les images Docker sont dans le registry, pas besoin d'artifacts

Rust cargo workspace:
- Cargo compile incrÃ©mentalement
- Mais sans cache local (emptyDir), on perd cet avantage
- D'oÃ¹ l'importance de cargo-chef cook

BuildKit inline cache limitations:
- Doit pull l'image pour avoir le cache
- Moins efficace que remote cache S3/GCS
- Mais plus simple Ã  setup

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Generated: 2025-12-30
Agent: Claude Sonnet 4.5 (DevOps Expert Analysis)
Context: feat-build-with-buildkit branch
Runner: Kubernetes executor with emptyDir
